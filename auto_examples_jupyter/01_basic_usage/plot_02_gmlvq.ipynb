{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generalized Matrix LVQ (GMLVQ)\n",
    "\n",
    "Example of how to use GMLVQ `[1]`_ on the classic iris dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklvq import GMLVQ\n",
    "\n",
    "matplotlib.rc(\"xtick\", labelsize=\"small\")\n",
    "matplotlib.rc(\"ytick\", labelsize=\"small\")\n",
    "\n",
    "# Contains also the target_names and feature_names, which we will use for the plots.\n",
    "iris = load_iris()\n",
    "\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "feature_names = [name[:-5] for name in iris.feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "Scale the data and create a GLVQ object with, e.g., custom distance function, activation\n",
    "function and solver. See the API reference under documentation for defaults and other\n",
    "possible parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Sklearn's standardscaler to perform z-transform\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Compute (fit) and apply (transform) z-transform\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# The creation of the model object used to fit the data to.\n",
    "model = GMLVQ(\n",
    "    distance_type=\"adaptive-squared-euclidean\",\n",
    "    activation_type=\"swish\",\n",
    "    activation_params={\"beta\": 2},\n",
    "    solver_type=\"waypoint-gradient-descent\",\n",
    "    solver_params={\"max_runs\": 10, \"k\": 3, \"step_size\": np.array([0.1, 0.05])},\n",
    "    random_state=1428,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to fit the GMLVQ object to the data and use the predict method to make the\n",
    "predictions. Note that this example only works on the training data and therefor does not say\n",
    "anything about the generalizability of the fitted model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       0.98      0.96      0.97        50\n",
      "           2       0.96      0.98      0.97        50\n",
      "\n",
      "    accuracy                           0.98       150\n",
      "   macro avg       0.98      0.98      0.98       150\n",
      "weighted avg       0.98      0.98      0.98       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the scaled data and true labels\n",
    "model.fit(data, labels)\n",
    "\n",
    "# Predict the labels using the trained model\n",
    "# predicting on the same trianing data?\n",
    "predicted_labels = model.predict(data)\n",
    "print(len(data))\n",
    "# To get a sense of the training performance we could print the classification report.\n",
    "print(classification_report(labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Relevance Matrix\n",
    "In addition to the prototypes (see GLVQ example), GMLVQ learns a\n",
    "matrix `lambda_` which can tell us something about which features are most relevant for the\n",
    "classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The relevance matrix is available after fitting the model.\n",
    "relevance_matrix = model.lambda_\n",
    "\n",
    "# Plot the diagonal of the relevance matrix\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Relevance Matrix Diagonal\")\n",
    "ax.bar(feature_names, np.diagonal(relevance_matrix))\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the relevance diagonal adds up to one. The most relevant features for\n",
    "distinguishing between the classes present in  the iris dataset seem to be (in decreasing\n",
    "order) the petal length, petal width, sepal length, and sepal width. Although not very\n",
    "interesting for the iris dataset one could use this information to select only the top most\n",
    "relevant features to be used for the classification and thus reducing the dimensionality of\n",
    "the problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "In addition to making predictions GMLVQ can be used to transform the data using the\n",
    "eigenvectors of the relevance matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data (scaled by square root of eigenvalues \"scale = True\")\n",
    "transformed_data = model.transform(data, scale=True)\n",
    "\n",
    "x_d = transformed_data[:, 0]\n",
    "y_d = transformed_data[:, 1]\n",
    "\n",
    "# Transform the model, i.e., the prototypes (scaled by square root of eigenvalues \"scale = True\")\n",
    "transformed_model = model.transform(model.prototypes_, scale=True)\n",
    "\n",
    "x_m = transformed_model[:, 0]\n",
    "y_m = transformed_model[:, 1]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Discriminative projection Iris data and GMLVQ prototypes\")\n",
    "colors = [\"blue\", \"red\", \"green\"]\n",
    "for i, cls in enumerate(model.classes_):\n",
    "    ii = cls == labels\n",
    "    ax.scatter(\n",
    "        x_d[ii],\n",
    "        y_d[ii],\n",
    "        c=colors[i],\n",
    "        s=100,\n",
    "        alpha=0.7,\n",
    "        edgecolors=\"white\",\n",
    "        label=iris.target_names[model.prototypes_labels_[i]],\n",
    "    )\n",
    "ax.scatter(x_m, y_m, c=colors, s=180, alpha=0.8, edgecolors=\"black\", linewidth=2.0)\n",
    "ax.set_xlabel(\"First eigenvector\")\n",
    "ax.set_ylabel(\"Second eigenvector\")\n",
    "ax.legend()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformed data and prototypes can be used to visualize the problem in a lower dimension,\n",
    "which is also the space the model would compute the distance. The axis are the directions which\n",
    "are the most discriminating directions (combinations of features). Hence, inspecting the\n",
    "eigenvalues and eigenvectors (axis) themselves can be interesting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the eigenvalues of the eigenvectors of the relevance matrix.\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Eigenvalues\")\n",
    "ax.bar(range(0, len(model.eigenvalues_)), model.eigenvalues_)\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.grid(False)\n",
    "\n",
    "# Plot the first two eigenvectors of the relevance matrix, which  is called `omega_hat`.\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"First Eigenvector\")\n",
    "ax.bar(feature_names, model.omega_hat_[:, 0])\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.grid(False)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"Second Eigenvector\")\n",
    "ax.bar(feature_names, model.omega_hat_[:, 1])\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plots from the eigenvalues and eigenvector we see a similar effects as we could see from\n",
    "just the diagonal of `lambda_`. The two leading (most relevant or discriminating) eigenvectors\n",
    "mostly use the petal length and petal width in their calculation. The diagonal of the\n",
    "relevance matrix can therefor be considered as a summary of the relevances of the features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "_`[1]` Schneider, P., Biehl, M., & Hammer, B. (2009). \"Adaptive Relevance Matrices in Learning\n",
    "Vector Quantization\" Neural Computation, 21(12), 3532â€“3561, 2009.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
